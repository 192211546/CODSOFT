# ==========================================
# IMAGE CAPTIONING AI (CNN + LSTM)
# ==========================================

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from PIL import Image

# ------------------------------
# LOAD PRE-TRAINED CNN (ResNet)
# ------------------------------
resnet = ResNet50(weights='imagenet')
resnet = Model(resnet.input, resnet.layers[-2].output)

# ------------------------------
# IMAGE FEATURE EXTRACTION
# ------------------------------
def extract_features(image_path):
    image = load_img(image_path, target_size=(224, 224))
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    image = preprocess_input(image)
    feature = resnet.predict(image, verbose=0)
    return feature

# ------------------------------
# SAMPLE TRAINING DATA
# ------------------------------
captions = [
    "startseq a dog running in the park endseq",
    "startseq a dog playing with ball endseq",
    "startseq a dog is running outside endseq"
]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(captions)
vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(c.split()) for c in captions)

# ------------------------------
# MODEL ARCHITECTURE
# ------------------------------
# Image Feature Input
image_input = Input(shape=(2048,))
image_layer = Dropout(0.5)(image_input)
image_layer = Dense(256, activation='relu')(image_layer)

# Text Input
text_input = Input(shape=(max_length,))
text_layer = Embedding(vocab_size, 256, mask_zero=True)(text_input)
text_layer = Dropout(0.5)(text_layer)
text_layer = LSTM(256)(text_layer)

# Combine
decoder = add([image_layer, text_layer])
decoder = Dense(256, activation='relu')(decoder)
output = Dense(vocab_size, activation='softmax')(decoder)

model = Model(inputs=[image_input, text_input], outputs=output)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# ------------------------------
# CAPTION GENERATION
# ------------------------------
def generate_caption(image_path):
    feature = extract_features(image_path)
    caption = "startseq"

    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([caption])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([feature, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = tokenizer.index_word.get(yhat)

        if word is None:
            break

        caption += " " + word
        if word == "endseq":
            break

    return caption.replace("startseq", "").replace("endseq", "")

# ------------------------------
# TEST
# ------------------------------
image_path = "test.jpg"  # Put any image here
print("Generated Caption:")
print(generate_caption(image_path))
